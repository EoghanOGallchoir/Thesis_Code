{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Dataset_Parser.ipynb","provenance":[],"mount_file_id":"1McjFxfPOtk_geY1JqZTOsaMhpaPApPHt","authorship_tag":"ABX9TyNEvP7jj9TXtvDIb37Kes7q"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","metadata":{"id":"dXMqWheAqoIi","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1629391414377,"user_tz":-60,"elapsed":220,"user":{"displayName":"Owen Gallagher","photoUrl":"","userId":"03483597507600674899"}},"outputId":"c0ee86bf-a8cd-4e40-ebb7-09463a880125"},"source":["%cd drive/MyDrive/Thesis\\ Datasets\n","import csv\n","import string\n","import warnings\n","warnings.filterwarnings('ignore')"],"execution_count":1,"outputs":[{"output_type":"stream","text":["/content/drive/MyDrive/Thesis Datasets\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"QB4qyai-IBD9","executionInfo":{"status":"ok","timestamp":1629391420812,"user_tz":-60,"elapsed":4240,"user":{"displayName":"Owen Gallagher","photoUrl":"","userId":"03483597507600674899"}},"outputId":"5571014f-c83a-48f2-e273-e3fb435caf5a"},"source":["!pip install pysentiment2"],"execution_count":2,"outputs":[{"output_type":"stream","text":["Collecting pysentiment2\n","  Downloading pysentiment2-0.1.1-py3-none-any.whl (1.9 MB)\n","\u001b[K     |████████████████████████████████| 1.9 MB 5.1 MB/s \n","\u001b[?25hRequirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (from pysentiment2) (1.1.5)\n","Requirement already satisfied: nltk>=2.0 in /usr/local/lib/python3.7/dist-packages (from pysentiment2) (3.2.5)\n","Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from nltk>=2.0->pysentiment2) (1.15.0)\n","Requirement already satisfied: numpy>=1.15.4 in /usr/local/lib/python3.7/dist-packages (from pandas->pysentiment2) (1.19.5)\n","Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.7/dist-packages (from pandas->pysentiment2) (2018.9)\n","Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas->pysentiment2) (2.8.2)\n","Installing collected packages: pysentiment2\n","Successfully installed pysentiment2-0.1.1\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"IVBSrxDagAz4"},"source":["def create_sentence_data(dataset, step):\n","  names = []\n","  positions = []\n","  ethnicity = []\n","  total = 0\n","  ids = []\n","  id = 0\n","  reports = []\n","  header = [\"ID\", \"Position\", \"Scouting Report Sentence(s)\", \"Race\"]\n","  with open(dataset) as csvfile:\n","    csvreader = csv.reader(csvfile)\n","    next(csvreader)\n","    for row in csvreader:\n","      id +=1\n","      sent = row[2].split(\".\")\n","      for i in range(0, len(sent)-1, step):\n","        ids.append(id)\n","        names.append(row[0])\n","        positions.append(row[1])\n","        sentence = str(sent[i:i+step]).translate(str.maketrans('', '', '[]\\''))\n","        reports.append(sentence)\n","        ethnicity.append(row[3])\n","  with open(str(step)+\"_sentence_data_2021.csv\", \"w\") as file:\n","    writer = csv.writer(file)\n","    writer.writerow(header)\n","    for i in range(0, len(names)):\n","      data = [ids[i], positions[i], reports[i], ethnicity[i]]\n","      writer.writerow(data)\n","        \n","create_sentence_data(\"dataset_2021_weka.csv\", 1)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"QV15KYpQtm08"},"source":["create_sentence_data(\"dataset_2021_weka.csv\", 2)      \n","create_sentence_data(\"dataset_2021_weka.csv\", 5)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"kURiAECBciOi","executionInfo":{"status":"ok","timestamp":1629393924554,"user_tz":-60,"elapsed":195,"user":{"displayName":"Owen Gallagher","photoUrl":"","userId":"03483597507600674899"}},"outputId":"ecf2ee20-8409-4a44-a1e7-27ab76f509a1"},"source":["from sklearn.model_selection import train_test_split, cross_val_score, RepeatedStratifiedKFold\n","from sklearn.feature_extraction.text import CountVectorizer\n","from sklearn.linear_model import LogisticRegression, SGDClassifier\n","from sklearn.metrics import confusion_matrix\n","from sklearn.feature_extraction.text import TfidfTransformer\n","from sklearn.naive_bayes import MultinomialNB\n","from sklearn import svm\n","from sklearn.pipeline import Pipeline\n","import numpy as np\n","from imblearn.over_sampling import RandomOverSampler\n","from imblearn.pipeline import Pipeline as imb_Pipeline\n","from numpy import mean\n","from nltk.stem import PorterStemmer\n","from nltk.tokenize import sent_tokenize, word_tokenize\n","import nltk\n","nltk.download('stopwords')\n","nltk.download('punkt')\n","from nltk.corpus import stopwords\n","import string"],"execution_count":39,"outputs":[{"output_type":"stream","text":["[nltk_data] Downloading package stopwords to /root/nltk_data...\n","[nltk_data]   Package stopwords is already up-to-date!\n","[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Package punkt is already up-to-date!\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"57844yL4LjNi","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1629393890180,"user_tz":-60,"elapsed":242,"user":{"displayName":"Owen Gallagher","photoUrl":"","userId":"03483597507600674899"}},"outputId":"c3026d6a-99b1-4601-d210-159a87a312d6"},"source":["def get_accuracy_oversampling(dataset):\n","\n","  reports = []\n","  eth = []\n","  ps = PorterStemmer()\n","  stop_words = set(stopwords.words('english'))\n","  with open(dataset) as csvfile:\n","    csvreader = csv.reader(csvfile)\n","    next(csvreader)\n","    for row in csvreader:\n","      # some preproccessing\n","      # removing stopwords\n","      word_tokens = word_tokenize(row[2])\n","      filtered = [w for w in word_tokens if not w.lower() in stop_words]\n","      filtered = []\n","      for w in word_tokens:\n","        if w not in stop_words:\n","          filtered.append(w)\n","      rep = \" \"\n","      rep = rep.join(filtered)\n","\n","      # removing punctuation\n","      rep.translate(str.maketrans('', '', string.punctuation))\n","      # stemming\n","      reports.append(ps.stem(rep))\n","      eth.append(row[3])\n","\n","\n","  rep_train, rep_test, y_train, y_test = train_test_split(reports, eth, test_size = 0.25)\n","\n","  vectorizer = CountVectorizer()\n","  vectorizer.fit(rep_train)\n","  X_train = vectorizer.transform(rep_train)\n","  X_test = vectorizer.transform(rep_test)\n","  \n","  # LR\n","  steps = [('over', RandomOverSampler()), ('model', LogisticRegression())]\n","  pipeline = imb_Pipeline(steps=steps)\n","  cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=0)\n","  scores = cross_val_score(pipeline, X_test, y_test, cv=cv)\n","  score = mean(scores)\n","  print(f\"LR accuracy: {round(score, 3)}.\")\n","\n","  # NB\n","  steps = [('over', RandomOverSampler()), ('model', MultinomialNB())]\n","  pipeline = imb_Pipeline(steps=steps)\n","  cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=0)\n","  scores = cross_val_score(pipeline, X_test, y_test, cv=cv, scoring='f1')\n","  score = mean(scores)\n","  print(f\"NB accuracy: {round(score, 3)}.\")\n","\n","  # SVM\n","  steps = [('over', RandomOverSampler()), ('model', svm.SVC())]\n","  pipeline = imb_Pipeline(steps=steps)\n","  cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=0)\n","  scores = cross_val_score(pipeline, X_test, y_test, cv=cv, scoring='f1')\n","  score = mean(scores)\n","  print(f\"SVM accuracy: {round(score, 3)}.\")\n","\n","  # SGD\n","  steps = [('over', RandomOverSampler()), ('model', SGDClassifier())]\n","  pipeline = imb_Pipeline(steps=steps)\n","  cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=0)\n","  scores = cross_val_score(pipeline, X_test, y_test, cv=cv, scoring='f1')\n","  score = mean(scores)\n","  print(f\"SGD accuracy: {round(score, 3)}.\")\n","\n","  "],"execution_count":37,"outputs":[{"output_type":"stream","text":["[nltk_data] Downloading package stopwords to /root/nltk_data...\n","[nltk_data]   Package stopwords is already up-to-date!\n","[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Package punkt is already up-to-date!\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ducxVXZrMjrL","executionInfo":{"status":"ok","timestamp":1629397519606,"user_tz":-60,"elapsed":19575,"user":{"displayName":"Owen Gallagher","photoUrl":"","userId":"03483597507600674899"}},"outputId":"c6f084a1-a0d5-4d7a-d745-25b94aff5dc3"},"source":["print(\"1 sentence accuracy...\")\n","get_accuracy_oversampling(\"1_sentence_data_2021.csv\")\n","print(\"\\n2 sentence accuracy...\")\n","get_accuracy_oversampling(\"2_sentence_data_2021.csv\")\n","print(\"\\n5 sentence accuracy...\")\n","get_accuracy_oversampling(\"5_sentence_data_2021.csv\")\n","print(\"\\nAll sentence accuracy...\")\n","get_accuracy_oversampling(\"dataset_2021_weka.csv\")"],"execution_count":43,"outputs":[{"output_type":"stream","text":["1 sentence accuracy...\n","LR accuracy: 0.735.\n","NB accuracy: 0.669.\n","SVM accuracy: 0.82.\n","SGD accuracy: 0.741.\n","\n","2 sentence accuracy...\n","LR accuracy: 0.745.\n","NB accuracy: 0.728.\n","SVM accuracy: 0.802.\n","SGD accuracy: 0.733.\n","\n","5 sentence accuracy...\n","LR accuracy: 0.796.\n","NB accuracy: 0.792.\n","SVM accuracy: 0.791.\n","SGD accuracy: 0.77.\n","\n","All sentence accuracy...\n","LR accuracy: 0.812.\n","NB accuracy: 0.831.\n","SVM accuracy: 0.831.\n","SGD accuracy: 0.84.\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"DyooWgwpH1ED","executionInfo":{"status":"ok","timestamp":1629391846168,"user_tz":-60,"elapsed":3451,"user":{"displayName":"Owen Gallagher","photoUrl":"","userId":"03483597507600674899"}}},"source":["# read in GI as key-value pairs with keys being preprocessed (famili, confid, unnecessari, etc)\n","import csv\n","import pysentiment2 as ps\n","import re\n","\n","def read_GI(filename):\n","  kvp = {}\n","  with open(filename ,'r', ) as file:\n","    csvreader = csv.reader(file)\n","    next(csvreader)\n","    next(csvreader)\n","    for col in csvreader:\n","      if col[1] != \"Lvd\": \n","        kvp[col[0]] = [str(word) for word in col[2:-2] if word]\n","  \n","  kvp2 = {}\n","  hiv4 = ps.HIV4()   \n","  for key in kvp:\n","    new_key = str(hiv4.tokenize(key))\n","    # print(new_key)\n","    #print(bool(new_key))\n","    res = re.findall(r'\\w+', new_key)\n","    # print(res)\n","    if res: \n","      #print(res[0])\n","      kvp2[res[0]] = kvp[key].copy()\n","    else:\n","      kvp2[new_key] = kvp[key].copy()\n","  return kvp2\n","\n","key_val = read_GI(\"inquirerbasic_py.csv\")"],"execution_count":12,"outputs":[]},{"cell_type":"code","metadata":{"id":"FR1uWLYDIKdn","executionInfo":{"status":"ok","timestamp":1629392141680,"user_tz":-60,"elapsed":11,"user":{"displayName":"Owen Gallagher","photoUrl":"","userId":"03483597507600674899"}}},"source":["# creating new csv's with GI sentiment for sentence(s)\n","def create_GI_sentiment_csv(dataset, gi_val):\n","  stop = 0\n","  tok_data = []\n","  names = []\n","  positions = []\n","  ethnicity = []\n","  total = 0\n","  ids = []\n","  id = 1\n","  header = [\"ID\", \"Position\", \"Scouting Report Sentence(s)\", \"Race\"]\n","  with open(dataset) as csv_file:\n","    csvreader = csv.reader(csv_file, delimiter=',')\n","    hiv4 = ps.HIV4()\n","    next(csvreader)\n","    for row in csvreader:\n","      tok_sent = []\n","      ids.append(row[0])\n","      names.append(row[0])\n","      positions.append(row[1])\n","      ethnicity.append(row[3])\n","\n","      text = row[2]\n","      tokens = hiv4.tokenize(text)\n","      for tok in tokens:\n","        for words in gi_val:\n","          if tok == words:\n","            #print(f\"{tok} senti is: {gi_val[words]}.\")\n","            tok_sent.append(gi_val[words])\n","      x = [' '.join(y) for y in tok_sent]\n","      row_string = \"\".join(x)\n","      tok_data.append(row_string)\n","    id+=1\n","  #print(tok_data)\n","  with open(\"gi_\"+str(dataset), \"w\") as file:\n","    writer = csv.writer(file)\n","    writer.writerow(header)\n","    for i in range(0, len(names)):\n","      #print(ids[i], positions[i], tok_data[i], ethnicity[i])\n","      data = [ids[i], positions[i], tok_data[i], ethnicity[i]]\n","      writer.writerow(data)\n","\n","\n"],"execution_count":23,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"yW9CHsPTv3GJ","executionInfo":{"status":"ok","timestamp":1629399056442,"user_tz":-60,"elapsed":225,"user":{"displayName":"Owen Gallagher","photoUrl":"","userId":"03483597507600674899"}},"outputId":"f077dfc6-d297-4510-aa2a-0da82172c3ad"},"source":["print(key_val[\"confid\"])"],"execution_count":46,"outputs":[{"output_type":"stream","text":["['Positiv', 'Pstv', 'Strong', 'Power', 'Pleasur', 'EMOT', 'Ovrst', 'WlbPsyc', 'WlbTot']\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"vqG4O7QIPMa5","executionInfo":{"status":"ok","timestamp":1629392178929,"user_tz":-60,"elapsed":35702,"user":{"displayName":"Owen Gallagher","photoUrl":"","userId":"03483597507600674899"}}},"source":["create_GI_sentiment_csv(\"1_sentence_data_2021.csv\", key_val)\n","create_GI_sentiment_csv(\"2_sentence_data_2021.csv\", key_val)\n","create_GI_sentiment_csv(\"5_sentence_data_2021.csv\", key_val)\n","create_GI_sentiment_csv(\"dataset_2021_weka.csv\", key_val)"],"execution_count":24,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"DFFMndORPkxu","executionInfo":{"status":"ok","timestamp":1629397606601,"user_tz":-60,"elapsed":27787,"user":{"displayName":"Owen Gallagher","photoUrl":"","userId":"03483597507600674899"}},"outputId":"1ecc1753-a620-48d0-b55a-52b68c2806d3"},"source":["print(\"1 sentence accuracy for GI tagged reports...\")\n","get_accuracy_oversampling(\"gi_1_sentence_data_2021.csv\")\n","print(\"\\n2 sentence accuracy for GI tagged reports...\")\n","get_accuracy_oversampling(\"gi_2_sentence_data_2021.csv\")\n","print(\"\\n5 sentence accuracy for GI tagged reports...\")\n","get_accuracy_oversampling(\"gi_5_sentence_data_2021.csv\")\n","print(\"\\nAll sentence accuracy for GI tagged reports...\")\n","get_accuracy_oversampling(\"gi_dataset_2021_weka.csv\")"],"execution_count":44,"outputs":[{"output_type":"stream","text":["1 sentence accuracy for GI tagged reports...\n","LR accuracy: 0.704.\n","NB accuracy: 0.661.\n","SVM accuracy: 0.784.\n","SGD accuracy: 0.698.\n","\n","2 sentence accuracy for GI tagged reports...\n","LR accuracy: 0.679.\n","NB accuracy: 0.673.\n","SVM accuracy: 0.75.\n","SGD accuracy: 0.687.\n","\n","5 sentence accuracy for GI tagged reports...\n","LR accuracy: 0.742.\n","NB accuracy: 0.802.\n","SVM accuracy: 0.781.\n","SGD accuracy: 0.777.\n","\n","All sentence accuracy for GI tagged reports...\n","LR accuracy: 0.794.\n","NB accuracy: 0.762.\n","SVM accuracy: 0.675.\n","SGD accuracy: 0.721.\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"f1tuPlubMo6x"},"source":[""],"execution_count":null,"outputs":[]}]}